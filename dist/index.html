<!DOCTYPE html>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="generator" content="Observable Framework v1.12.0">
<title>Differentiable Optimization of Similarity Scores Between Models and Brains</title>
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link rel="preload" as="style" href="https://fonts.googleapis.com/css2?family=Source+Serif+4:ital,opsz,wght@0,8..60,200..900;1,8..60,200..900&amp;display=swap" crossorigin>
<link rel="preload" as="style" href="./_observablehq/theme-air,near-midnight,wide.4b0e2451.css">
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css2?family=Source+Serif+4:ital,opsz,wght@0,8..60,200..900;1,8..60,200..900&amp;display=swap" crossorigin>
<link rel="stylesheet" type="text/css" href="./_observablehq/theme-air,near-midnight,wide.4b0e2451.css">
<link rel="modulepreload" href="./_observablehq/client.fa1da142.js">
<link rel="modulepreload" href="./_observablehq/runtime.674751e7.js">
<link rel="modulepreload" href="./_observablehq/stdlib.8048ec16.js">
<link rel="modulepreload" href="./_npm/d3@7.9.0/7055d4c5.js">
<link rel="modulepreload" href="./_npm/d3-array@3.2.4/e95f898e.js">
<link rel="modulepreload" href="./_npm/d3-axis@3.0.0/d44feff9.js">
<link rel="modulepreload" href="./_npm/d3-brush@3.0.0/5830b12a.js">
<link rel="modulepreload" href="./_npm/d3-chord@3.0.1/84d7b8e9.js">
<link rel="modulepreload" href="./_npm/d3-color@3.1.0/2c0cdfa2.js">
<link rel="modulepreload" href="./_npm/d3-contour@4.0.2/626bedc4.js">
<link rel="modulepreload" href="./_npm/d3-delaunay@6.0.4/00c41b5d.js">
<link rel="modulepreload" href="./_npm/d3-dispatch@3.0.1/b5f7cdc6.js">
<link rel="modulepreload" href="./_npm/d3-drag@3.0.0/b22c5864.js">
<link rel="modulepreload" href="./_npm/d3-dsv@3.0.1/407f7a1f.js">
<link rel="modulepreload" href="./_npm/d3-ease@3.0.1/6f15f633.js">
<link rel="modulepreload" href="./_npm/d3-fetch@3.0.1/ef1ec490.js">
<link rel="modulepreload" href="./_npm/d3-force@3.0.0/5e1ff060.js">
<link rel="modulepreload" href="./_npm/d3-format@3.1.0/5851d7ef.js">
<link rel="modulepreload" href="./_npm/d3-geo@3.1.1/dcd02767.js">
<link rel="modulepreload" href="./_npm/d3-hierarchy@3.1.2/f1db2593.js">
<link rel="modulepreload" href="./_npm/d3-interpolate@3.0.1/034b7bcb.js">
<link rel="modulepreload" href="./_npm/d3-path@3.1.0/4bb53638.js">
<link rel="modulepreload" href="./_npm/d3-polygon@3.0.1/bbafde58.js">
<link rel="modulepreload" href="./_npm/d3-quadtree@3.0.1/aa5b35a8.js">
<link rel="modulepreload" href="./_npm/d3-random@3.0.1/32c7fec2.js">
<link rel="modulepreload" href="./_npm/d3-scale@4.0.2/567840a0.js">
<link rel="modulepreload" href="./_npm/d3-scale-chromatic@3.1.0/cf9b720b.js">
<link rel="modulepreload" href="./_npm/d3-selection@3.0.0/5dcd62f4.js">
<link rel="modulepreload" href="./_npm/d3-shape@3.2.0/f8e03c56.js">
<link rel="modulepreload" href="./_npm/d3-time@3.1.0/5bc129e1.js">
<link rel="modulepreload" href="./_npm/d3-time-format@4.1.0/19c92b44.js">
<link rel="modulepreload" href="./_npm/d3-timer@3.0.1/f31b5398.js">
<link rel="modulepreload" href="./_npm/d3-transition@3.0.1/8debb4ba.js">
<link rel="modulepreload" href="./_npm/d3-zoom@3.0.0/4b0cc581.js">
<link rel="modulepreload" href="./_npm/internmap@2.0.3/5eed35fd.js">
<link rel="modulepreload" href="./_npm/delaunator@5.0.1/e67acb27.js">
<link rel="modulepreload" href="./_npm/robust-predicates@3.0.2/8ac9039b.js">
<script type="module">

import {define} from "./_observablehq/client.fa1da142.js";

define({id: "40f21020", outputs: ["d3"], body: async () => {
const d3 = await import("./_npm/d3@7.9.0/7055d4c5.js");

return {d3};
}});

</script>
<div id="observablehq-center">
<main id="observablehq-main" class="observablehq">
<div class="observablehq observablehq--block"><!--:40f21020:--></div>
<link href="https://fonts.googleapis.com/css?family=Google+Sans%7CNoto+Sans%7CCastoro" rel="stylesheet">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
<style>

.content {
  display: flex;
  flex-direction: column;
  /* Align items to the left */
  align-items: flex-start;
  /* Center the content horizontally */
  margin-left: auto;
  margin-right: auto;
  /* Set a maximum width for the content */
  max-width: 900px; 
  font-family: 'Noto Sans', sans-serif;
}

.content p, .content h3, .content h2, .content h1 {
  max-width: none;
}

.content h2 {
  margin-top: 30px;
  margin-bottom: 10px;
}

.content h3 {
  margin-top: 20px;
  margin-bottom: 10px;
}

.content p {
  text-align: justify;
  margin: 0;
}

.content .title, .content .authors, .content .affiliations{
  max-width: none;
  margin-right: auto;
  margin-left: auto;
  text-align: center;
}

.content .title {
  margin-top: 40px;
  font-size: 40px;
}

.content .authors {
  display: flex;
  justify-content: center; /* Center the authors horizontally */
  flex-wrap: wrap; /* Ensure the list wraps if space is insufficient */
  column-gap: 30px; /* Space between author names */
  row-gap: 10px; /* Space between author names */
  margin-top: 10px;
  margin-bottom: 0;
  font-style: normal;
  color: #444;
  text-align: center;
}

.content .affiliations {
  display: flex;
  justify-content: center; /* Center the authors horizontally */
  flex-wrap: wrap; /* Ensure the list wraps if space is insufficient */
  gap: 25px; /* Space between names */
  margin-top: 10px;
  margin-bottom: 10px;
  color: #888;
  text-align: center;
  font-size: smaller;
}

.content .abstract {
  max-width: 600px;
  margin: auto;
}

.content .observablehq--block {
  /* TODO?: margin: auto; */
  margin-top: auto;
  margin-bottom: auto;
}

.content .github-link {
  margin-top: 20px;
  text-align: center;
  margin: auto;
}

.content .github-link a {
  display: inline-flex;
  align-items: center;
  justify-content: center;
  background-color: #f5f5f5; /* Light color background */
  color: #333;
  padding: 8px 12px;
  border-radius: 5px;
  text-decoration: none;
  font-size: 16px;
  border: 1px solid #ccc; /* Optional: adds a subtle border */
  margin-top: 25px;
}

.content .github-link a:hover {
  background-color: #e9e9e9;
}

.content .github-link img {
  margin-right: 8px;
}


</style>
<div class="content">
<h1 class="title">Differentiable Optimization of Similarity Scores Between Models and Brains</h1>
<!-- <h2 class="authors">Nathan Cloos, Markus Siegel, Scott L. Brincat, Earl K. Miller, Guangyu Robert Yang, Christopher J. Cueva</h2> -->
<div class="authors">
  <span>Nathan Cloos<sup style="font-size: 10px">1</sup></span>
  <span>Moufan Li<sup style="font-size: 10px">2</sup></span>
  <span>Markus Siegel<sup style="font-size: 10px">3</sup></span>
  <span>Scott L. Brincat<sup style="font-size: 10px">1</sup></span>
  <span>Earl K. Miller<sup style="font-size: 10px">1</sup></span>
  <span>Guangyu Robert Yang<sup style="font-size: 10px">1</sup></span>
  <span>Christopher J. Cueva<sup style="font-size: 10px">1</sup></span>
</div>
<div class="affiliations">
  <p><sup style="font-size: 10px">1</sup>MIT BCS</p>
  <p><sup style="font-size: 10px">2</sup>NYU</p>
  <p><sup style="font-size: 10px">3</sup>HIH Tübingen</p>
</div>
<div style="margin-left: auto; margin-right: auto; display: flex; column-gap: 20px">
  <div class="github-link">
    <a href="https://arxiv.org/pdf/2407.07059" target="_blank" rel="noopener noreferrer">
      <i class="fa fa-file-pdf-o" style="padding-right: 8px; padding-top: 2px; padding-left: 1px"></i>
      paper
    </a>
  </div>
  <div class="github-link">
    <a href="https://www.youtube.com/live/urOMQGGdhCM?feature=shared&amp;t=6383" target="_blank" rel="noopener noreferrer">
      <i class="fa fa-video-camera" style="padding-right: 8px; padding-top: 2px; padding-left: 1px"></i>
      video
    </a>
  </div>
  <div class="github-link">
    <a href="https://github.com/nacloos/diffscore" target="_blank" rel="noopener noreferrer">
      <img style="padding-top: 3px" src="https://icons.iconarchive.com/icons/papirus-team/papirus-apps/256/github-icon.png" alt="GitHub" width="20">
      code
    </a>
  </div>
  <!-- <div class="github-link">
    <a href="" target="_blank">
      <img style="padding-top: 3px"  src="https://icons.iconarchive.com/icons/papirus-team/papirus-apps/256/github-icon.png" alt="GitHub" width="20">
      similarity package
    </a>
  </div> -->
</div>
<div style="margin-left: auto; margin-right: auto;">
</div>
<!-- <img style="margin: auto; margin-top: 4em; margin-bottom: 2em;" src="./data/intro.png" width="650" /> -->
<img style="margin: auto; margin-top: 4em; margin-bottom: 2em;" src="./_file/data/animations/intro.374c304d.gif" width="560">
<iframe style="margin: auto; margin-top: 2em; margin-bottom: 2em;" width="560" height="315" src="https://www.youtube.com/embed/urOMQGGdhCM?start=6385" title="Presentation" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>
<!-- ## Abstract -->
<!-- <h2 class="abstract" style="margin-top: 50px; margin-bottom: 10px">Abstract</h2>
<p class="abstract" style="margin-bottom: 20px">
How do we know if two systems – biological or artificial – process information in a similar way? Similarity measures such as linear regression, Centered Kernel Alignment (CKA), Normalized Bures Similarity (NBS), and angular Procrustes distance, are often used to quantify this similarity. However, it is currently unclear what drives high similarity scores and even what constitutes a “good” score. Here, we introduce a novel tool to investigate these questions by differentiating through similarity measures to directly maximize the score. Surprisingly, we find that high similarity scores do not guarantee that models encode task-relevant information in a manner consistent with neural data; and this is particularly acute for CKA and even some variations of cross-validated and regularized linear regression. We find that there is no universal threshold for a good similarity score—it depends on both the measure and the dataset. In addition, synthetic datasets optimized to maximize similarity scores initially learn the highest variance principal component of the target dataset, but some methods like angular Procrustes capture lower variance dimensions much earlier than methods like CKA. To shed light on this, we mathematically derive the sensitivity of CKA, angular Procrustes, and NBS to the variance of principal component dimensions, and explain the emphasis CKA places on high variance components. Finally, by jointly optimizing multiple similarity measures, we characterize their allowable ranges and reveal that some similarity measures are more constraining than others: a high angular Procrustes similarity implies a high CKA score, but not the converse. While current measures offer a seemingly straightforward way to quantify the similarity between neural systems, our work underscores the need for carefully interpreting similarity scores. In addition, we hope that the tools we developed will be used by practitioners to better understand current and future similarity measures.
</p> -->
<!-- 
## Content
<ul style="margin: 0;">
  <li><a href="#optimization-of-similarity-scores">Optimization of Similarity Scores</a></li>
  <li><a href="#scores-to-capture-principal-components">Scores to Capture Principal Components</a></li>
  <li><a href="#high-scores-failing-to-encode-task-variables">High Scores Failing to Encode Task Variables</a></li>
  <li><a href="#metric-cards">Metric Cards</a></li>
  <li><a href="#similarity-package">Similarity Package</a></li>
</ul> -->
<h2 id="introduction" tabindex="-1"><a class="observablehq-header-anchor" href="#introduction">Introduction</a></h2>
<p>How do we know if two systems – biological or artificial – process information in a similar way? Similarity measures such as linear regression, Centered Kernel Alignment (CKA), Normalized Bures Similarity (NBS), and angular Procrustes distance, are often used to quantify this similarity. However, it is currently unclear what drives high similarity scores and even what constitutes a “good” score. Here, we introduce a novel tool to investigate these questions by differentiating through similarity measures to directly maximize the score.</p>
<br>
To evaluate the similarity of representations between two systems, we extract feature representations such as activity in a brain area or model layer in response to some sample stimuli. Our objective is to quantify the alignment between these representations using a similarity score. We define a scoring function score(X, Y) as a measure that increases with similarity, achieving a maximum of 1 when X = Y.
<h2 id="optimization-of-similarity-scores" tabindex="-1"><a class="observablehq-header-anchor" href="#optimization-of-similarity-scores">Optimization of similarity scores</a></h2>
<p>We identify what drives high similarity scores by differentiating through similarity measures to directly maximize the score.
Given a target dataset X, for example a neural dataset, we initialize a synthetic dataset Y by randomly sampling from a standard Gaussian distribution with the same shape as X. We optimize Y to maximize the similarity score with X, leveraging the differentiability of the similarity measures.</p>
<img style="margin: auto; margin-top: 1em; margin-bottom: 2em;" src="./_file/data/score_grad_ascent.ec24578c.png" width="290">
<!-- 
TODO: error loading tex.js when deployed
```tex
Y_{k+1}  = Y_k + \alpha \dfrac{\partial}{\partial Y} \text{score}(X, Y_k)
``` -->
<p>We visualize the optimization process for a few similarity measures below. The dataset on the left is the reference dataset X, and the datasets on the right are the synthetic datasets Y for different similarity measures.</p>
<div style="display: flex; justify-content: space-around; align-items: center; margin: auto;">
  <div style="text-align: center; width: 200px;">
    <p style="text-align: center">Reference</p>
    <img src="./_file/data/animations/reference.b03048ff.gif" width="150">
  </div>
  <div style="text-align: center; width: 250px;">
    <p style="text-align: center">Ridge Regression</p>
    <img src="./_file/data/animations/ridge.a8b34a74.gif" width="150">
  </div>
  <div style="text-align: center; width: 250px;">
    <p style="text-align: center">CKA</p>
    <img src="./_file/data/animations/cka.3ecf8281.gif" width="150">
  </div>
  <div style="text-align: center; width: 200px;">
    <p style="text-align: center">Angular Procrustes</p>
    <img src="./_file/data/animations/procrustes-angular-score.f803b5b3.gif" width="150">
  </div>
</div>
<p>We also evaluate how each principal component (PC) of the reference dataset X is captured as Y is optimized for similarity. What do we learn? CKA can be near its maximum value even when only the first principal component is captured.</p>
<img style="margin: auto; margin-top: 1em; margin-bottom: 2em;" src="./_file/data/fig_pc.68857013.png">
<p>In the <a href="https://arxiv.org/pdf/2407.07059" target="_blank" rel="noopener noreferrer">paper</a>, we present theoretical analyses to explain these results. In particular, we mathematically derive the sensitivity of CKA, angular Procrustes, and Normalized Bures Similarity to the variance of principal component dimensions, and explain the dependence CKA shows to high variance components.</p>
<h2 id="what-is-a-good-similarity-score" tabindex="-1"><a class="observablehq-header-anchor" href="#what-is-a-good-similarity-score">What is a good similarity score?</a></h2>
<p>Our approach to address this question is to examine, across five neural datasets, the similarity score required for a synthetic dataset to encode task relevant information to the same degree as the neural data.</p>
<img style="margin: auto; margin-top: 1em; margin-bottom: 2em;" src="./_file/data/fig3.474f0ff2.png">
<p>What is a good value for a similarity score? There is no absolute answer! An angular Procrustes score above 0.5 may constitute a good score for the Mante 2013 dataset but a score above 0.8 is required for the Siegel 2015 dataset. And this also depends on the similarity measure.</p>
<br>
Another crucial point to make with the figure above is that high similarity scores near the maximum value of 1, particularly for CKA and unregularized linear regression without cross-validation, do not guarantee that models encode task-relevant information in a manner consistent with neural data, i.e. the CKA and linear regression curves in the Siegel 2015 dataset do not approach the horizontal line showing the decode accuracy for the neural data. There may be important features in a dataset that are not captured by a model even when the model-data similarity score is high.
<h3 id="ridge-regression" tabindex="-1"><a class="observablehq-header-anchor" href="#ridge-regression">Ridge Regression</a></h3>
<p>Let's take a closer look at ridge regression scores, examining how they change as we vary the regularization strength λ.</p>
<img style="margin: auto; margin-top: 1em; margin-bottom: 2em;" src="./_file/data/figS2.303e58c3.png">
<p>This figure highlights that for linear regression methods, including those that are cross-validated and regularized, a high similarity score does not always guarantee that task-relevant information is encoded in a manner consistent with the neural data. In other words, high similarity scores can be achieved without fully capturing the important features of the neural representations.</p>
<br>
This underscores the need for caution when interpreting similarity scores, especially in the context of comparing artificial and biological neural representations. It suggests that while similarity measures like ridge regression are useful tools, they should be used in conjunction with other analyses to ensure a comprehensive understanding of how well a model captures the key aspects of neural data.
<h2 id="are-metrics-mutually-independent" tabindex="-1"><a class="observablehq-header-anchor" href="#are-metrics-mutually-independent">Are metrics mutually independent?</a></h2>
<p>A natural question that arises is whether different similarity metrics are independent of each other, or if they exhibit consistent relationships. To address this, we consider three possible relationships between any two given similarity metrics:</p>
<div style="display: flex; align-items: center; justify-content: space-between;">
  <div style="flex: 1;">
    <ul>
      <li><strong>Independent:</strong> A high score on one metric does not guarantee a high score on the other.</li>
      <li><strong>Coupled:</strong> A high score on one metric implies a high score on the other, and vice versa.</li>
      <li><strong>Asymmetric:</strong> A high score on the first metric guarantees a high score on the second, but not the other way around.</li>
    </ul>
  </div>
  <div style="flex: 1;">
    <img src="./_file/data/fig7a.4589a12e.png" width="300" style="display: block; margin: auto;">
  </div>
</div>
<p>We characterize the allowable range of scores between two different similarity measures by jointly optimizing their scores.</p>
<img style="margin: auto; margin-top: 1em; margin-bottom: 2em;" src="./_file/data/fig7b.1135015e.png" width="500">
<p>The figure above shows that a high angular Procrustes similarity implies a high CKA score, but not the converse. A high value of angular Procrustes implies a high score for unregularized linear regression but linear regression that is regularized and cross-validated across experimental conditions can take independent values.</p>
<h2 id="standardizing-similarity-measures" tabindex="-1"><a class="observablehq-header-anchor" href="#standardizing-similarity-measures">Standardizing similarity measures</a></h2>
<p>We are also developing a Python package to make it easier to compare similarity measures across studies. We currently have around 100 different similarity measures from 14 papers registered and standardized to a common interface.</p>
<div style="margin-left: auto; margin-right: auto; display: flex; column-gap: 20px">
  <div class="github-link">
    <a href="https://arxiv.org/pdf/2409.18333" target="_blank" rel="noopener noreferrer">
      <i class="fa fa-file-pdf-o" style="padding-right: 8px; padding-top: 2px; padding-left: 1px"></i>
      package paper
    </a>
  </div>
  <div class="github-link">
    <a href="https://github.com/nacloos/similarity-repository" target="_blank" rel="noopener noreferrer">
      <img style="padding-top: 3px" src="https://icons.iconarchive.com/icons/papirus-team/papirus-apps/256/github-icon.png" alt="GitHub" width="20">
      package code
    </a>
  </div>
</div>
<br>
<img style="margin-top: 10px" src="./_file/data/implemented_measures.097fd35d.png" width="1000">
<!-- ## Scores to Capture Principal Components
Different similarity measures differentially prioritize learning principal components of
the data.
<div style="margin-bottom: 10px"></div>

```js
const pcDatasets = new Map([
  ["Ultrametric",       FileAttachment("data/benchmarks/ultrametric/scores_at_threshold.csv").csv({typed: true})],
  // ["siegel15-V4-var99", FileAttachment("data/benchmarks/siegel15-V4-var99/scores_at_threshold.csv").csv({typed: true})],
  ["MajajHong2015",     FileAttachment("data/benchmarks/MajajHong2015/scores_at_threshold.csv").csv({typed: true})],
  ["FreemanZiemba2013", FileAttachment("data/benchmarks/FreemanZiemba2013/scores_at_threshold.csv").csv({typed: true})],
  ["Hatsopoulos2007",   FileAttachment("data/benchmarks/Hatsopoulos2007/scores_at_threshold.csv").csv({typed: true})],
  ["Mante2013",         FileAttachment("data/benchmarks/Mante2013/scores_at_threshold.csv").csv({typed: true})],
])
const pcDatasetIds = Array.from(pcDatasets.keys());
const pcDataset = view(Inputs.select(pcDatasets, {label: "Dataset"}));
```

```js
const pcMeasures = pcDataset.map(d => d.measure).filter((v, i, a) => a.indexOf(v) === i);
const pcSelectedMeasures = view(Inputs.checkbox(
  pcMeasures, {
    label: "Measures",
    value: ["cka", "procrustes-angular-score"],
    sort: true
  }
));
```

```js
const pcFilteredScores = pcDataset.filter(d => pcSelectedMeasures.includes(d.measure));
const pcPlotScores = Plot.plot({
  color: {legend: true},
  x: {
    label: "PC Explained Variance",
    type: "log",
    tickFormat: ",",
    grid: true,
    // nice: true,
    // ticks: 10
  },
  y: {
    label: "Score to Capture PC",
    grid: true,
    nice: true,
    ticks: 5
  },
  marks: [
    Plot.line(
      pcFilteredScores, 
      {
        x: "pc_explained_variance",
        y: "scores_at_threshold",
        stroke: "measure",
        tip: true,
        strokeWidth: 4
    })
  ],
  style: {
    fontSize: "13px",
    width: "500px",
  }
})

if (pcFilteredScores.length === 0) {
  display(html` `);
} else {
  display(pcPlotScores);
}
``` -->
<!-- 
## High Scores Failing to Encode Task Variables
Optimizing for similarity scores reveals model datasets with high scores that fail to encode all the relevant task variables.

<div style="margin-bottom: 20px"></div>

```js
// it seems that FileAttachment requires the path to be give as a single string literal
const datasets = new Map([
  ["Ultrametric",       FileAttachment("data/benchmarks/ultrametric/scores_vs_decoding_acc.csv").csv({typed: true})],
//  ["siegel15-V4-var99", FileAttachment("data/siegel15-V4-var99.csv").csv({typed: true})],
  ["MajajHong2015",     FileAttachment("data/benchmarks/MajajHong2015/scores_vs_decoding_acc.csv").csv({typed: true})],
  ["FreemanZiemba2013", FileAttachment("data/benchmarks/FreemanZiemba2013/scores_vs_decoding_acc.csv").csv({typed: true})],
  ["Hatsopoulos2007",   FileAttachment("data/benchmarks/Hatsopoulos2007/scores_vs_decoding_acc.csv").csv({typed: true})],
  ["Mante2013",         FileAttachment("data/benchmarks/Mante2013/scores_vs_decoding_acc.csv").csv({typed: true})],
]);
const datasetIds = Array.from(datasets.keys());
```

```js
const selectedData = view(Inputs.select(datasets, {label: "Dataset"}))
```

<div style="margin-bottom: 8px"></div>

```js
// select task variables to decodeµ
// get all the column names that start with "decode." and rename to remove the prefix
const taskVariables = Object.keys(selectedData[0])
  .filter(key => key.startsWith("decode."))
  .map(key => key.replace("decode.", ""));
const selectedVariable = view(Inputs.select(
  taskVariables, {
    label: "Decode Variable",
    value: taskVariables,
    sort: true
  }
));
```
<div style="margin-bottom: 8px"></div>

```js
const defaultMeasures = ["cka", "procrustes-angular-score"]
// const measures = scores.map(d => d.measure).filter((v, i, a) => a.indexOf(v) === i);
const measures = selectedData.map(d => d.measure).filter((v, i, a) => a.indexOf(v) === i);
const selectedMeasures = view(Inputs.checkbox(
  measures, {
    label: "Measures",
    value: defaultMeasures,
    sort: true
  }
));
```

```js
// const scores = datasetScores.ultrametric;
// const filteredScores = scores.filter(d => selectedMeasures.includes(d.measure));
const filteredScores = selectedData.filter(d => selectedMeasures.includes(d.measure));
const plotScores = Plot.plot({
  color: {legend: true},
  x: {
    label: "Score",
    grid: true,
  },
  y: {
    label: "Decoding Accuracy",
    grid: true,
  },
  marks: [
    Plot.line(
      filteredScores, 
      {
        x: "score",
        // y: "decoding_accuracy",
        y: `decode.${selectedVariable}`,
        stroke: "measure",
        tip: true,
        strokeWidth: 4
    })
  ],
  style: {
    fontSize: "13px",
    width: "500px",
  }
})

if (filteredScores.length === 0) {
  display(html` `);
} else {
  display(plotScores);
}
``` -->
<!-- ## Metric Cards
### Invariances Properties


```js
const measureCards = FileAttachment("data/cards/measures.csv").csv({typed: true});
```

```js
view(Inputs.table(
  measureCards, {
    columns: [
      "id",
      "name",
      "permutation",
      "orthogonal",
      "isotropic-scaling",
      "invertible-linear",
      "translation",
      "affine"
    ],
    header: {
      "permutation": "PT",
      "orthogonal": "OT",
      "isotropic-scaling": "IS",
      "invertible-linear": "ILT",
      "translation": "TR",
      "affine": "AT"
    },
    format: {
      "permutation": formatBoolean,
      "orthogonal": formatBoolean,
      "invertible-linear": formatBoolean,
      "isotropic-scaling": formatBoolean,
      "translation": formatBoolean,
      "affine": formatBoolean
    },
    sort: "id",
    layout: "auto"
  }
));

function formatBoolean(x) {
  if (x === "True") {
    return htl.html`✔`;  
    // return htl.html`✅`;
  } else {
    // return htl.html`✘`;
    return "";
  }
}
``` -->
<!-- 
<div style="margin-top: 10px; margin-bottom: 25px">

<p style="font-size: 14px">Invariance classes:</p>
<ul style="font-size: 14px; list-style-type: none; margin: 0;">
  <li><b>PT</b>: Permutation Transformation</li>
  <li><b>OT</b>: Orthogonal Transformation</li>
  <li><b>ILT</b>: Invertible Linear Transformation</li>
  <li><b>IS</b>: Isotropic Scaling</li>
  <li><b>TR</b>: Translation</li>
  <li><b>AT</b>: Affine Transformation</li>
<ul>
</div> -->
<!-- TODO: Metric Relations -->
<!-- TODO: solve inconsistencies across backends -->
<!-- In the figure below, we compare our implementations (diffscore) to other already published implementations.
<a href="https://github.com/nacloos/similarity-repository">
  https://github.com/nacloos/similarity-repository
</a> -->
<!-- <img src="data/cards/backends_matrix.png" width="1000" /> -->
<!-- TODO: relationships between measures? -->
<!-- D3 heatmap is quite low-level and require a lot of work, use seaborn instead -->
<!-- ```js
const backends = FileAttachment("data/cards/backends.csv").csv({typed: true});
```

```js
// https://d3-graph-gallery.com/graph/heatmap_style.html
const data = backends;
const width = 1000;
const height = 350;
const color = d3.scaleOrdinal(d3.schemeObservable10);

// append the svg object to the body of the page
// const svg = d3.create("svg")
//   .attr("width", width + margin.left + margin.right)
//   .attr("height", height + margin.top + margin.bottom)
// .append("g")
//   .attr("transform", `translate(${margin.left}, ${margin.top})`);

const svg = d3.create("svg")
    .attr("width", width)
    .attr("height", height)
    .attr("viewBox", [150, 0, width, height+200])
    // .attr("style", "max-width: 100%; height: auto;");

//Read the data

// Labels of row and columns -> unique identifier of the column called 'group' and 'variable'
const myGroups = Array.from(new Set(data.map(d => d.group)))
const myVars = Array.from(new Set(data.map(d => d.variable)))

// Build X scales and axis:
const x = d3.scaleBand()
  .range([ 0, width ])
  .domain(myGroups)
  .padding(0.05);
// svg.append("g")
//   .style("font-size", 15)
//   .attr("transform", `translate(0, ${height})`)
//   .call(d3.axisBottom(x).tickSize(0))
//   .select(".domain").remove()
svg.append("g")
  .style("font-size", 25)
  .attr("transform", `translate(0, ${height})`)
  .call(d3.axisBottom(x).tickSize(0))
  .selectAll("text")
    .style("text-anchor", "end")
    .attr("dx", "-.8em")
    .attr("dy", ".15em")
    .attr("transform", "rotate(-90)");

svg.select(".domain").remove();


// Build Y scales and axis:
const y = d3.scaleBand()
  .range([ height, 0 ])
  .domain(myVars)
  .padding(0.05);
svg.append("g")
  .style("font-size", 30)
  .call(d3.axisLeft(y).tickSize(0))
  .select(".domain").remove()


// take data min value excluding -1
const dataMin = d3.min(data, d => d.value === -1 ? Infinity : d.value);
const dataMax = d3.max(data, d => d.value);
// Build color scale
// const myColor = d3.scaleSequential()
//   .interpolator(d3.interpolateWarm)
//   .domain([dataMax, dataMin])

// linear scale from green to red
const myColor = d3.scaleLinear()
  .range(["green", "red"])
  .domain([dataMin, dataMax])

// create a tooltip
const tooltip = d3.select("#my_dataviz")
  .append("div")
  .style("opacity", 0)
  .attr("class", "tooltip")
  .style("background-color", "white")
  .style("border", "solid")
  .style("border-width", "2px")
  .style("border-radius", "5px")
  .style("padding", "5px")

// Three function that change the tooltip when user hover / move / leave a cell
const mouseover = function(event,d) {
  tooltip
    .style("opacity", 1)
  d3.select(this)
    .style("stroke", "black")
    .style("opacity", 1)
}
const mousemove = function(event,d) {
  tooltip
    .html("The exact value of<br>this cell is: " + d.value)
    .style("left", (event.x)/2 + "px")
    .style("top", (event.y)/2 + "px")
}
const mouseleave = function(event,d) {
  tooltip
    .style("opacity", 0)
  d3.select(this)
    .style("stroke", "none")
    .style("opacity", 0.8)
}

// add the squares
svg.selectAll()
  .data(data, function(d) {return d.group+':'+d.variable;})
  .join("rect")
    .attr("x", function(d) { return x(d.group) })
    .attr("y", function(d) { return y(d.variable) })
    .attr("rx", 4)
    .attr("ry", 4)
    .attr("width", x.bandwidth() )
    .attr("height", y.bandwidth() )
    .style("fill", function(d) { 
      // if -1 corresponds to nan, return transparent hex code
      if (d.value === -1) {
        return "#00000000";
      }    
      return myColor(d.value)
    } )
    .style("stroke-width", 4)
    .style("stroke", "none")
    .style("opacity", 0.8)
  .on("mouseover", mouseover)
  .on("mousemove", mousemove)
  .on("mouseleave", mouseleave)


// Add title to graph
// svg.append("text")
//         .attr("x", 0)
//         .attr("y", 0)
//         .attr("text-anchor", "left")
//         .style("font-size", "22px")
//         .text("A d3.js heatmap");

// // Add subtitle to graph
// svg.append("text")
//         .attr("x", 0)
//         .attr("y", -20)
//         .attr("text-anchor", "left")
//         .style("font-size", "14px")
//         .style("fill", "grey")
//         .style("max-width", 400)
//         .text("A short description of the take-away message of this chart.");

// display(svg.node())

``` -->
</div>
</main>
<footer id="observablehq-footer">
<div>Built with <a href="https://observablehq.com/" target="_blank" rel="noopener noreferrer">Observable</a> on <a title="2024-10-21T12:00:00">Oct 21, 2024</a>.</div>
</footer>
</div>
